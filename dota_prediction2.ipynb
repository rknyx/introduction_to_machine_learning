{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "RADIANT_WIN = 'radiant_win'\n",
    "ADDITIONAL_RESULT_FIELDS = ['duration', 'tower_status_radiant', 'tower_status_dire', 'barracks_status_radiant', 'barracks_status_dire']\n",
    "RESULT_FIELDS = [RADIANT_WIN] + ADDITIONAL_RESULT_FIELDS\n",
    "\n",
    "#1) read table from csv, remove result fields\n",
    "#====================================\n",
    "features_filename = '/home/rk/submissions/final_work/features.csv'\n",
    "index_col = 'match_id'\n",
    "features = pandas.read_csv(features_filename, index_col=index_col)\n",
    "\n",
    "def remove_resulting_columns(_df):\n",
    "    global RADIANT_WIN\n",
    "    return _df[list(set(_df.columns.values.tolist()) - set(RESULT_FIELDS))], _df[RADIANT_WIN]\n",
    "\n",
    "X,Y = remove_resulting_columns(features)\n",
    "#===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dire_bottle_time, radiant_first_ward_time, radiant_flying_courier_time, dire_courier_time, first_blood_team, dire_flying_courier_time, first_blood_time, dire_first_ward_time, first_blood_player1, first_blood_player2, radiant_bottle_time, radiant_courier_time\n"
     ]
    }
   ],
   "source": [
    "#====================================\n",
    "#2) Get epmty values  \n",
    "\n",
    "def get_features_with_misses(_df):\n",
    "    return [_x for _x in _df.columns.values if len(_df[pandas.isnull(_df[_x])]) > 0]\n",
    "\n",
    "len(get_features_with_misses(X))\n",
    "# 12 columns with missing data\n",
    "# first_blood_* <- nan if there was no 'first blood event' during first 5 minutes\n",
    "# radiant_bottle_time <- wasn't bought\n",
    "# radiant_courier_time <- wasn't bought\n",
    "# radiant_flying_courier_time <- wasn't bought\n",
    "# the same is for dire\n",
    "#====================================\n",
    "print \", \".join(get_features_with_misses(remove_resulting_columns(features)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#====================================\n",
    "#3) Fill empty values\n",
    "X = X.fillna(0)\n",
    "#====================================\n",
    "\n",
    "#====================================\n",
    "#4) Target variable column is 'radiant_win'\n",
    "#===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66820321,  0.66402863,  0.6670671 ,  0.66105802,  0.66454867])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to calc for 20 trees\n",
      "start to calc for 30 trees\n",
      "start to calc for 40 trees\n",
      "start to calc for 50 trees\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "TREES_COUNT = 30\n",
    "TIMINGS = []\n",
    "\n",
    "cv = KFold(len(X), n_folds=5, shuffle=True)\n",
    "#calc cross val score for several trees count\n",
    "\n",
    "classifier_scores = {}\n",
    "for trees_num in [20, 30, 40, 50]:\n",
    "    print 'start to calc for %s trees' % trees_num\n",
    "    classifier = GradientBoostingClassifier(n_estimators=trees_num)\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    _scores = cross_val_score(estimator=classifier, X=X, y=Y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    final_time = datetime.datetime.now() - start_time\n",
    "    classifier_scores[trees_num] = {\n",
    "        'scores': _scores,\n",
    "        'time': final_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees: '40', time: '0:02:08.995060', roc_auc: '0.69402516975'\n",
      "trees: '50', time: '0:02:40.961570', roc_auc: '0.697246250333'\n",
      "trees: '20', time: '0:01:08.258663', roc_auc: '0.681777441497'\n",
      "trees: '30', time: '0:01:38.068877', roc_auc: '0.689439163418'\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "#print results\n",
    "_scores = deepcopy(classifier_scores)\n",
    "for tree_num in _scores:\n",
    "    _scores[tree_num]['scores'] = np.mean(_scores[tree_num]['scores'])\n",
    "    print \"trees: '%s', time: '%s', roc_auc: '%s'\" % (tree_num, str(_scores[tree_num]['time']), _scores[tree_num]['scores'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready func 'simple_regression_scaled' with c='0.0001', res='0.711217320974', time='0:00:02.903926'\n",
      "ready func 'simple_regression_scaled' with c='0.001', res='0.716167292924', time='0:00:05.213528'\n",
      "ready func 'simple_regression_scaled' with c='0.01', res='0.716344879872', time='0:00:06.721026'\n",
      "ready func 'simple_regression_scaled' with c='0.1', res='0.71632046854', time='0:00:07.015689'\n",
      "ready func 'simple_regression_scaled' with c='1.0', res='0.716317570129', time='0:00:07.126337'\n",
      "ready func 'simple_regression_scaled' with c='10.0', res='0.716317186619', time='0:00:07.024566'\n",
      "ready func 'simple_regression_scaled' with c='100.0', res='0.716317112405', time='0:00:07.034862'\n",
      "ready func 'simple_regression_scaled' with c='1000.0', res='0.71631709547', time='0:00:07.225902'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.71634487987202744, '0:00:06.721026', 0.01)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X)\n",
    "regression = LogisticRegression()\n",
    "\n",
    "C_CONSTANTS = np.power(10.0, range(-4,4))\n",
    "\n",
    "def search_c(func, c_list):\n",
    "    def calc_with_time(func, c):\n",
    "        _start_time = datetime.datetime.now()\n",
    "        _res = func(c)\n",
    "        _final_time = str(datetime.datetime.now() - _start_time)\n",
    "        print \"ready func '%s' with c='%s', res='%s', time='%s'\" % (func.__name__, c, _res, _final_time)\n",
    "        return _res, _final_time, c\n",
    "        \n",
    "    return max(map(lambda x: calc_with_time(func, x), c_list), key=lambda x: x[0])\n",
    "\n",
    "#1) logistic regression on all features        \n",
    "def simple_regression_scaled(c):\n",
    "    global x_scaled, Y, cv\n",
    "    regression = LogisticRegression(C=c)\n",
    "    _res = cross_val_score(estimator=regression, X=x_scaled, y=Y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    return np.mean(_res)\n",
    "    \n",
    "simple_regression_scaled_res = search_c(simple_regression_scaled, C_CONSTANTS)\n",
    "simple_regression_scaled_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready func 'regression_wout_categorized_features' with c='0.0001', res='0.711191345793', time='0:00:02.705892'\n",
      "ready func 'regression_wout_categorized_features' with c='0.001', res='0.716179492122', time='0:00:04.926753'\n",
      "ready func 'regression_wout_categorized_features' with c='0.01', res='0.716364860708', time='0:00:06.530423'\n",
      "ready func 'regression_wout_categorized_features' with c='0.1', res='0.71634202932', time='0:00:06.624340'\n",
      "ready func 'regression_wout_categorized_features' with c='1.0', res='0.716338118433', time='0:00:06.723832'\n",
      "ready func 'regression_wout_categorized_features' with c='10.0', res='0.71633758659', time='0:00:06.729390'\n",
      "ready func 'regression_wout_categorized_features' with c='100.0', res='0.716337503947', time='0:00:06.634843'\n",
      "ready func 'regression_wout_categorized_features' with c='1000.0', res='0.716337537839', time='0:00:06.830869'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.71636486070808325, '0:00:06.530423', 0.01)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_heroes_features = ['%s%s_hero' % (_k, _i) for _i in range(1, 6) for _k in ('r', 'd')]\n",
    "\n",
    "CATEGORIZED_FEATURES = ['lobby_type'] + _heroes_features\n",
    "\n",
    "x_wout_categorized = X[list(set(X.columns.values.tolist()) - set(CATEGORIZED_FEATURES))]\n",
    "x_wout_categorized_scaled = StandardScaler().fit_transform(x_wout_categorized)\n",
    "\n",
    "#2) logistic regression on all features wout categorized    \n",
    "def regression_wout_categorized_features(c):\n",
    "    global x_wout_categorized_scaled, Y, cv\n",
    "    regression_wout_categorized = LogisticRegression(C=c)\n",
    "    return np.mean(cross_val_score(estimator=regression_wout_categorized, X=x_wout_categorized_scaled, y=Y, cv=cv,\n",
    "                           scoring='roc_auc', n_jobs=-1))\n",
    "\n",
    "regression_wout_categorized_features_res = search_c(regression_wout_categorized_features, C_CONSTANTS)\n",
    "regression_wout_categorized_features_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total heroes count is:'112'\n"
     ]
    }
   ],
   "source": [
    "heroes_dict_filename = '/home/rk/submissions/final_work/data/dictionaries/heroes.csv'\n",
    "heroes = pandas.read_csv(heroes_dict_filename, index_col='id')\n",
    "total_heroes_count = heroes.count()[0]\n",
    "print \"total heroes count is:'%s'\" % total_heroes_count\n",
    "\n",
    "def calc_used_heroes(_df):\n",
    "    _res = set()\n",
    "    for _col_name in _heroes_features:\n",
    "        _res = _res.union(set(_df[_col_name].unique().tolist()))\n",
    "    return _res\n",
    "\n",
    "#3) heroes count\n",
    "heroes_count = len(calc_used_heroes(X))\n",
    "#4) regression with word bag\n",
    "def calc_heroes_words_bag(_df, heroes_count):\n",
    "    X_pick = np.zeros((_df.shape[0], heroes_count))\n",
    "    for i, match_id in enumerate(_df.index):\n",
    "        for p in xrange(5):\n",
    "            X_pick[i, _df.ix[match_id, 'r%d_hero' % (p+1)]-1] = 1\n",
    "            X_pick[i, _df.ix[match_id, 'd%d_hero' % (p+1)]-1] = -1\n",
    "    return X_pick\n",
    "\n",
    "word_bag = calc_heroes_words_bag(X, total_heroes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready func 'regression_with_word_bag' with c='0.0001', res='0.724937232498', time='0:00:03.302896'\n",
      "ready func 'regression_with_word_bag' with c='0.001', res='0.746165535297', time='0:00:06.225842'\n",
      "ready func 'regression_with_word_bag' with c='0.01', res='0.751573468685', time='0:00:09.929529'\n",
      "ready func 'regression_with_word_bag' with c='0.1', res='0.751767993492', time='0:00:13.434582'\n",
      "ready func 'regression_with_word_bag' with c='1.0', res='0.751748739393', time='0:00:15.947667'\n",
      "ready func 'regression_with_word_bag' with c='10.0', res='0.751745749678', time='0:00:14.941429'\n",
      "ready func 'regression_with_word_bag' with c='100.0', res='0.751745529351', time='0:00:14.056328'\n",
      "ready func 'regression_with_word_bag' with c='1000.0', res='0.751745450835', time='0:00:14.757150'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.75176799349225965, '0:00:13.434582', 0.10000000000000001)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled_with_word_bag = np.concatenate((x_wout_categorized_scaled, word_bag), axis=1)\n",
    "    \n",
    "def regression_with_word_bag(c):\n",
    "    global x_scaled_with_word_bag, cv, Y\n",
    "    return np.mean(cross_val_score(\n",
    "        estimator=LogisticRegression(C=c),\n",
    "        X=x_scaled_with_word_bag, y=Y, cv=cv, scoring='roc_auc', n_jobs=-1))\n",
    "\n",
    "regression_with_word_bag_res = search_c(regression_with_word_bag, C_CONSTANTS)\n",
    "regression_with_word_bag_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1753393 ,  0.8246607 ],\n",
       "       [ 0.24283038,  0.75716962],\n",
       "       [ 0.81221233,  0.18778767],\n",
       "       ..., \n",
       "       [ 0.76600267,  0.23399733],\n",
       "       [ 0.37529702,  0.62470298],\n",
       "       [ 0.57289783,  0.42710217]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5) get prediction on test data\n",
    "test_filename = '/home/rk/submissions/final_work/features_test.csv'\n",
    "test_data = pandas.read_csv(test_filename, index_col=index_col)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "test_x_wout_categorized = test_data[list(set(test_data.columns.values.tolist()) - set(CATEGORIZED_FEATURES))]\n",
    "test_data_wout_categorized_scaled = StandardScaler().fit_transform(test_x_wout_categorized)\n",
    "\n",
    "word_bag_test = calc_heroes_words_bag(test_data, total_heroes_count)\n",
    "x_scaled_with_word_bag_test = np.concatenate((test_data_wout_categorized_scaled, word_bag_test), axis=1)\n",
    "\n",
    "best_classifier = LogisticRegression(C=0.1)\n",
    "best_classifier.fit(X=x_scaled_with_word_bag, y=Y)\n",
    "prediction = best_classifier.predict_proba(x_scaled_with_word_bag_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991419367389\n",
      "0.00354077701738\n"
     ]
    }
   ],
   "source": [
    "print prediction[:,0:1].max()\n",
    "print prediction[:,0:1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
