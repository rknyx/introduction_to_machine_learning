{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv('~/submissions/scores/classification.csv')\n",
    "data\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "true_positive = 0\n",
    "true_negative = 0\n",
    "\n",
    "true = data['true']\n",
    "pred = data['pred']\n",
    "\n",
    "for i in xrange(0, len(true)):\n",
    "    if true[i] == 0:\n",
    "        if pred[i] == 0:\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "    if true[i] == 1:\n",
    "        if pred[i] == 0:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            true_positive += 1\n",
    "        \n",
    "answ = [true_positive, false_positive, false_negative, true_negative]\n",
    "\n",
    "with open(\"/home/rk/submissions/scores/answer1.txt\", \"w\") as file:\n",
    "    file.write(' '.join(map(str,answ)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54, 0.56, 0.42, 0.48]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "answ2 = [\n",
    "    accuracy_score(true, pred),\n",
    "    precision_score(true, pred),\n",
    "    recall_score(true, pred),\n",
    "    f1_score(true, pred)\n",
    "]\n",
    "\n",
    "answ2 = [\n",
    "    0.54, 0.56, 0.42, 0.48\n",
    "]\n",
    "print answ2\n",
    "with open(\"/home/rk/submissions/scores/answer2.txt\", \"w\") as file:\n",
    "    file.write(' '.join(map(str,answ2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = pandas.read_csv('~/submissions/scores/scores2.csv')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "columns = [\"score_logreg\", \"score_svm\", \"score_knn\", \"score_tree\"]\n",
    "_max = -1000000\n",
    "_max_name = \"\"\n",
    "for col in columns:\n",
    "    res = roc_auc_score(data2['true'], data2[col])\n",
    "    if res > _max:\n",
    "        _max = res\n",
    "        _max_name = col\n",
    "        \n",
    "_max, _max_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "max_pres = 0\n",
    "max_pres_name = \"\"\n",
    "target_recall = 0\n",
    "for col in columns:\n",
    "    res = precision_recall_curve(data2['true'], data2[col])\n",
    "    pres = res[0]\n",
    "    recall = res[1]\n",
    "    threshold = res[2]\n",
    "    for i in xrange(0, len(pres)):\n",
    "        if recall[i] < 0.7:\n",
    "            continue\n",
    "        if pres[i] > max_pres:\n",
    "            max_pres = pres[i]\n",
    "            max_pres_name = col\n",
    "            target_recall = recall[i]\n",
    "\n",
    "max_pres, max_pres_name, target_recall\n",
    "with open(\"/home/rk/submissions/scores/answer4.txt\", \"w\") as file:\n",
    "    file.write(max_pres_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
